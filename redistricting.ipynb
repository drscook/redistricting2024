{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed202c23-2ec2-4aad-8733-a16c01165349",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-13T12:23:06.028555Z",
     "iopub.status.busy": "2024-10-13T12:23:06.028410Z",
     "iopub.status.idle": "2024-10-13T12:23:15.081023Z",
     "shell.execute_reply": "2024-10-13T12:23:15.080702Z",
     "shell.execute_reply.started": "2024-10-13T12:23:06.028547Z"
    }
   },
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "import warnings, os, pathlib, shutil, copy, requests, zipfile, dataclasses, us, hashlib, humanfriendly as hf, codetiming\n",
    "import numpy as np, pandas as pd, geopandas as gpd, networkx as nx, cudf, cugraph as cugr, h5py\n",
    "import matplotlib.pyplot as plt, seaborn as sns, plotly.express as px\n",
    "\n",
    "#################################################\n",
    "### Convenience and helpers ###\n",
    "#################################################\n",
    "os.environ['PROJ_LIB']  = '/home/pythonserver/miniconda3/envs/rapids/share/proj'\n",
    "path_root = pathlib.Path('/home/pythonserver/gerrymandering')\n",
    "path_in = path_root / 'input'\n",
    "path_out = path_root / 'output'\n",
    "[warnings.filterwarnings(action='ignore', message=f\".*{w}.*\") for w in [\n",
    "    \"Could not infer format, so each element will be parsed individually, falling back to `dateutil`\",\n",
    "    \"Engine has switched to 'python' because numexpr does not support extension array dtypes\",\n",
    "    \"Specify dtype option on import or set low_memory=False\",\n",
    "    \"The default of observed=False is deprecated and will be changed to True in a future version of pandas\",\n",
    "    \"errors='ignore' is deprecated and will raise in a future version\",\n",
    "    \"Multi is deprecated and the removal of multi edges will no longer be supported from 'symmetrize'\",\n",
    "]]\n",
    "def restart(self):\n",
    "    self._start_time = None\n",
    "    self.start()\n",
    "codetiming.Timer.restart = restart\n",
    "timer = codetiming.Timer(text=hf.format_timespan)\n",
    "\n",
    "\n",
    "def listify(*args):\n",
    "    if len(args)==1:\n",
    "        if args[0] in [None, np.nan, '']:\n",
    "            return list()\n",
    "        elif isinstance(args[0], str):\n",
    "            return [args[0]]\n",
    "    try:\n",
    "        return list(*args)\n",
    "    except:\n",
    "        return list(args)\n",
    "\n",
    "def pattern(x, y, pre=None, suf=None):\n",
    "    p = [len(z) for z in str(y).split('.')]\n",
    "    pre = str(pre if pre is not None else ' ')\n",
    "    suf = str(suf if suf is not None else 0)\n",
    "    x = [str(int(x))] if len(p)==1 else str(round(float(x), p[1])).split('.')\n",
    "    x[0] = x[0].rjust(p[0], pre)\n",
    "    if len(p) > 1:\n",
    "        x[1] = x[1].ljust(p[1], suf)[:p[1]]\n",
    "    return '.'.join(x)\n",
    "\n",
    "now = lambda: str(pd.Timestamp.utcnow())[:-13].replace(' ','_')\n",
    "drop = lambda src, drp: type(src)({k:v for k,v in (src if isinstance(src,dict) else dict(zip(src,src))).items() if k not in listify(drp)})\n",
    "join = lambda x, s=' ': str(s).join([str(y) for y in listify(x)])\n",
    "\n",
    "\n",
    "def disp(X, rows=6, cols=None):\n",
    "    with pd.option_context('display.min_rows', rows, 'display.max_rows', rows, 'display.max_columns', cols):\n",
    "        display(X)\n",
    "pd.DataFrame.disp = disp\n",
    "pd.Series.disp = disp\n",
    "\n",
    "\n",
    "def convert(ser, bools=[], cat=False, force_int64=True, dtype_backend='numpy_nullable'):\n",
    "    \"\"\"overly complicated preprocessor for dtype tricks\"\"\"\n",
    "    ser = ser.convert_dtypes(dtype_backend)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        if isinstance(ser.dtype, pd.BooleanDtype) or pd.api.types.is_datetime64_any_dtype(ser) or pd.api.types.is_timedelta64_dtype(ser):  #no action\n",
    "            pass\n",
    "        elif pd.api.types.is_numeric_dtype(ser):  #attempt to downcast numerical\n",
    "            ser = pd.to_numeric(ser, downcast='unsigned')\n",
    "        elif pd.api.types.is_string_dtype(ser) or pd.api.types.is_object_dtype(ser):  #prep strings\n",
    "            ser = ser.astype('string')\n",
    "            try:  # is it a datetime  hiding as string?\n",
    "                ser = pd.to_datetime(ser)\n",
    "            except:  # is it a number hiding as string?\n",
    "                try:\n",
    "                    ser = pd.to_numeric(ser, downcast='unsigned')\n",
    "                except:  # it must be a real string - lower case it and fill blanks with pd.NA\n",
    "                    ser = ser.str.lower().replace('', pd.NA)\n",
    "                    if cat:  # make pandas categorical if cat=True\n",
    "                        ser = ser.astype('category')\n",
    "    if force_int64 and pd.api.types.is_integer_dtype(ser):  # to_numeric likes to downcast to Int 32 or Int16, which often causes problems later, so explicitly upcast to Int64\n",
    "        ser = ser.astype('Int64')\n",
    "    if ser.name in bools:  # make boolean if in list of bools\n",
    "        ser = (ser == ser.max()).astype('boolean').fillna(False)\n",
    "    return ser.convert_dtypes(dtype_backend)\n",
    "\n",
    "\n",
    "def prep(X, **kwargs):\n",
    "    f = lambda s: s.lower().replace(' ','_').replace('-','_') if isinstance(s, str) else s  # column renamer\n",
    "    g = lambda x: gpd.GeoDataFrame(x).rename(columns=f).apply(convert, **kwargs)  # ensure X is a geodataframe, rename its columns, and apply convert to its columns\n",
    "    df = g(X)\n",
    "    idx = g(df[[]].reset_index())  # drop columns, reset_index to move index to columns, then apply g\n",
    "    df = df.set_index(pd.MultiIndex.from_frame(idx))  # set idx back to df's index\n",
    "    Z = df.squeeze() if isinstance(X, pd.Series) else df  # squeeze to series if input was series\n",
    "#     display(type(Z), Z.shape, Z.dtypes)\n",
    "    return Z\n",
    "pd.DataFrame.prep = prep\n",
    "pd.Series.prep = prep\n",
    "gpd.GeoDataFrame.prep = prep\n",
    "gpd.GeoSeries.prep = prep\n",
    "\n",
    "\n",
    "def get_mlt(S, grp='geoid20'):\n",
    "    A = S / S.groupby(grp).transform('sum').replace(0,pd.NA)\n",
    "    N = 1 / S.groupby(grp).transform('size')\n",
    "    return A.combine_first(N).rename('mlt').to_frame().prep()\n",
    "pd.DataFrame.get_mlt = get_mlt\n",
    "pd.Series.get_mlt = get_mlt\n",
    "\n",
    "\n",
    "def apply_mlt(df, mlt='mlt', col=None):\n",
    "    mlt = df.pop(mlt).to_frame().values\n",
    "    col = df.columns.tolist() if col is None else listify(col)\n",
    "    df[col] *= mlt\n",
    "    return df\n",
    "pd.DataFrame.apply_mlt = apply_mlt\n",
    "\n",
    "\n",
    "def rm(path, root=True):\n",
    "    if path.is_file():\n",
    "        p.unlink()\n",
    "    elif path.is_dir():\n",
    "        if root:\n",
    "            shutil.rmtree(path)\n",
    "        else:\n",
    "            for p in path.iterdir():\n",
    "                rm(p)\n",
    "    # else:\n",
    "    #     print(path, 'not found')\n",
    "\n",
    "\n",
    "def fetch(url, fn):\n",
    "    fn = pathlib.Path(fn)\n",
    "    fn.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if not fn.exists():\n",
    "        print(url)\n",
    "        with open(fn, \"wb\") as f:\n",
    "            f.write(requests.get(url).content)\n",
    "    return fn\n",
    "\n",
    "\n",
    "def fetch_zip(url, fn, path=None):\n",
    "    fn = fetch(url, fn)\n",
    "    zipfile.ZipFile(fn).extractall(fn.parent if path is None else path)\n",
    "    return fn\n",
    "\n",
    "\n",
    "def load_h5py(fn, **kwargs):\n",
    "    with h5py.File(fn, 'r', **kwargs) as f:\n",
    "        dct = {k: v[:] for k,v in f.items()}\n",
    "    return dct\n",
    "\n",
    "\n",
    "def load(fn, **kwargs):\n",
    "    prep_kwargs = {k:kwargs.pop(k) for k in ['bools','cat'] if k in kwargs}\n",
    "    for read in [gpd.read_parquet, pd.read_parquet, pd.read_csv, gpd.read_file, load_h5py]:\n",
    "        try:\n",
    "            X = read(fn, **kwargs)\n",
    "            break\n",
    "        except:\n",
    "            X = None\n",
    "    try:\n",
    "        return X.prep(**prep_kwargs)\n",
    "    except:\n",
    "        return X\n",
    "\n",
    "def dump(fn, X, **kwargs):\n",
    "    fn = pathlib.Path(fn)\n",
    "    fn.parent.mkdir(parents=True, exist_ok=True)\n",
    "    X = X.prep(cat=True)\n",
    "    X.to_parquet(fn, **kwargs)\n",
    "    return X\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class MyBaseClass():\n",
    "    def __contains__(self, key):\n",
    "        return hasattr(self, key)\n",
    "    def __getitem__(self, key):\n",
    "        return getattr(self, key)\n",
    "    def __setitem__(self, key, val):\n",
    "        setattr(self, key, val)\n",
    "    def __delitem__(self, key):\n",
    "        delattr(self, key)\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Redistrict(MyBaseClass):\n",
    "    st: str = 'tx'\n",
    "    years: tuple = tuple(range(2014,2023))\n",
    "    dst: str = 'sd'\n",
    "    overwrite: tuple = ()\n",
    "\n",
    "    # def pq(self, x):\n",
    "    #     pq = path_out / f\"{self.st}/{x}/{x}.parquet\"\n",
    "    #     pq.parent.mkdir(parents=True, exist_ok=True)\n",
    "    #     return pq\n",
    "\n",
    "    \n",
    "    def pq(self, *x):\n",
    "        pq = path_out / f\"{self.st}/{join(x,'/')}/{x[-1]}.parquet\"\n",
    "        pq.parent.mkdir(parents=True, exist_ok=True)\n",
    "        return pq\n",
    "\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.state = us.states.lookup(self.st).name\n",
    "        self.fips  = us.states.lookup(self.st).fips\n",
    "        self.src = self.pq('src').parent\n",
    "        self.depend = {\n",
    "            'blk_shp': 'vtd_shp',\n",
    "            'blk_vtd': ['vtd_shp','blk_shp'],\n",
    "            'blk': ['pl','elec','plan','blk_shp','blk_vtd'],\n",
    "            'edge': 'blk',\n",
    "            'cnty': 'blk',\n",
    "            'vtd': 'blk',\n",
    "            'cd': 'blk',\n",
    "            'sd': 'blk',\n",
    "            'hd': 'blk',\n",
    "            'ed': 'blk',\n",
    "            'piece': 'blk',\n",
    "            'vote': ['elec','piece'],\n",
    "            'graph': ['blk','edge'],\n",
    "            'chain': 'graph',\n",
    "            'blk_simp': 'blk',\n",
    "        }\n",
    "        for x in ['overwrite','years']:\n",
    "            self[x] = listify(self[x])\n",
    "        # for x in {'pl','blk_vtd','cnty','vtd','cd','sd','hd','ed'}.intersection(self.overwrite):\n",
    "        #     confirm = input(f\"Are you sure you want to overwrite {x} (y/n)\")\n",
    "        #     if confirm.lower() != 'y':\n",
    "        #         self.overwrite.remove(x)\n",
    "        for x in self.overwrite:\n",
    "            rm(self.pq(x).parent)\n",
    "\n",
    "\n",
    "    def get(self, x, **kwargs):\n",
    "        if x not in self:\n",
    "            fn = self.pq(self.dst, x)\n",
    "            # print(fn)\n",
    "            self[x] = load(fn)\n",
    "            if self[x] is None:\n",
    "                for y in listify(self.depend.get(x)):\n",
    "                    self.get(y)\n",
    "                timer.restart()\n",
    "                print('create', x)\n",
    "                self[x] = self[f'get_{x}'](fn=fn, **kwargs)\n",
    "                if x not in ['graph','chain']:\n",
    "                    self[x] = self[x].prep(cat=True)\n",
    "                    dump(fn, self[x])\n",
    "                timer.stop()\n",
    "            # print('loaded', x)\n",
    "        # display(self[nm].reset_index().dtypes)\n",
    "\n",
    "\n",
    "    def get_pl(self, fn=None):\n",
    "        s = self.src / f\"2020_PLSummaryFile_FieldNames.xlsx\"\n",
    "        u = f\"https://www2.census.gov/programs-surveys/decennial/rdo/about/2020-census-program/Phase3/SupportMaterials/{s.name}\"\n",
    "        fetch(u, s)\n",
    "        headers = pd.read_excel(s, sheet_name=None)\n",
    "        z = self.src / f\"{self.st}2020.pl.zip\"\n",
    "        u = f\"https://www2.census.gov/programs-surveys/decennial/2020/data/01-Redistricting_File--PL_94-171/{self.state}/{z.name}\"\n",
    "        fetch_zip(u, z, fn.parent)\n",
    "        M = 1.0 * np.array([\n",
    "            [1,0,0,0,0,0],\n",
    "            [0,1,0,0,0,0],\n",
    "            [0,0,1,0,0,0],\n",
    "            [0,0,0,1,0,0],\n",
    "            [0,0,0,0,1,0],\n",
    "            [0,0,0,0,0,1],\n",
    "\n",
    "            [1,1,0,0,0,0],\n",
    "            [1,0,1,0,0,0],\n",
    "            [1,0,0,1,0,0],\n",
    "            [1,0,0,0,1,0],\n",
    "            [1,0,0,0,0,1],\n",
    "            [0,1,1,0,0,0],\n",
    "            [0,1,0,1,0,0],\n",
    "            [0,1,0,0,1,0],\n",
    "            [0,1,0,0,0,1],\n",
    "            [0,0,1,1,0,0],\n",
    "            [0,0,1,0,1,0],\n",
    "            [0,0,1,0,0,1],\n",
    "            [0,0,0,1,1,0],\n",
    "            [0,0,0,1,0,1],\n",
    "            [0,0,0,0,1,1],\n",
    "\n",
    "            [1,1,1,0,0,0],\n",
    "            [1,1,0,1,0,0],\n",
    "            [1,1,0,0,1,0],\n",
    "            [1,1,0,0,0,1],\n",
    "            [1,0,1,1,0,0],\n",
    "            [1,0,1,0,1,0],\n",
    "            [1,0,1,0,0,1],\n",
    "            [1,0,0,1,1,0],\n",
    "            [1,0,0,1,0,1],\n",
    "            [1,0,0,0,1,1],\n",
    "            [0,1,1,1,0,0],\n",
    "            [0,1,1,0,1,0],\n",
    "            [0,1,1,0,0,1],\n",
    "            [0,1,0,1,1,0],\n",
    "            [0,1,0,1,0,1],\n",
    "            [0,1,0,0,1,1],\n",
    "            [0,0,1,1,1,0],\n",
    "            [0,0,1,1,0,1],\n",
    "            [0,0,1,0,1,1],\n",
    "            [0,0,0,1,1,1],\n",
    "\n",
    "            [1,1,1,1,0,0],\n",
    "            [1,1,1,0,1,0],\n",
    "            [1,1,1,0,0,1],\n",
    "            [1,1,0,1,1,0],\n",
    "            [1,1,0,1,0,1],\n",
    "            [1,1,0,0,1,1],\n",
    "            [1,0,1,1,1,0],\n",
    "            [1,0,1,1,0,1],\n",
    "            [1,0,1,0,1,1],\n",
    "            [1,0,0,1,1,1],\n",
    "            [0,1,1,1,1,0],\n",
    "            [0,1,1,1,0,1],\n",
    "            [0,1,1,0,1,1],\n",
    "            [0,1,0,1,1,1],\n",
    "            [0,0,1,1,1,1],\n",
    "\n",
    "            [1,1,1,1,1,0],\n",
    "            [1,1,1,1,0,1],\n",
    "            [1,1,1,0,1,1],\n",
    "            [1,1,0,1,1,1],\n",
    "            [1,0,1,1,1,1],\n",
    "            [0,1,1,1,1,1],\n",
    "\n",
    "            [1,1,1,1,1,1],\n",
    "        ])\n",
    "        M /= M.sum(axis=1, keepdims=True)\n",
    "        kwargs = {'delimiter':'|','encoding_errors':'ignore'}\n",
    "        idx = ['stusab','chariter','logrecno']\n",
    "        I = load(fn.parent / f\"{self.st}geo2020.pl\", **kwargs, names=headers[f\"2020 P.L. Geoheader Fields\"].columns, usecols=[1,2,5,7,9]).query('sumlev==750').drop(columns='sumlev').set_index(idx)\n",
    "        usecols = drop(list(range(7,76)), [13,14,30,51,67,74])\n",
    "        L = [I.join(\n",
    "                load(fn.parent / f\"{self.st}0000{i}2020.pl\", **kwargs, names=headers[f\"2020 P.L. Segment {i} Fields\"].columns, usecols=[1,2,4,*[j+k for k in usecols]]).set_index(idx)\n",
    "            ).set_index('geocode').rename_axis('geoid20').prep() @ M\n",
    "             for i in [1,2] for j in [0,73]]\n",
    "        dct = dict(zip(['all','nonhisp','vap','vap_nonhisp'], L))\n",
    "        dct['hisp'] = dct['all'] - dct['nonhisp']\n",
    "        dct['vap_hisp'] = dct['vap'] - dct['vap_nonhisp']\n",
    "        return pd.concat([v.assign(t=v.sum(axis=1)).set_axis([f'{k}_{x}' for x in ['white','black','native','asian','islander','other','total']], axis=1) for k,v in dct.items()], axis=1)\n",
    "\n",
    "\n",
    "    def get_elec(self, fn=None):\n",
    "        dct = {yr: fn.parent / f\"{yr}_General_Election_Returns.csv\" for yr in self.years}\n",
    "        if not all(s.exists() for s in dct.values()):\n",
    "            z = self.src / f\"2022-general-vtds-election-data.zip\"\n",
    "            u = f\"https://data.capitol.texas.gov/dataset/35b16aee-0bb0-4866-b1ec-859f1f044241/resource/b9ebdbdb-3e31-4c98-b158-0e2993b05efc/download/{z.name}\"\n",
    "            fetch_zip(u, z, fn.parent)\n",
    "        lst = [load(s, usecols=['vtdkeyvalue','FIPS','County','Office','Name','Party','Incumbent','Votes']).assign(year=yr) for yr, s in dct.items() if s.exists()]\n",
    "        return pd.concat(lst).prep(bools='incumbent').rename(columns={'vtdkeyvalue':'vtd','fips':'cnty'}).set_index(['vtd','year','office','party'])\n",
    "\n",
    "\n",
    "    def get_plan(self, fn=None):\n",
    "        U = {\n",
    "            \"c\": \"https://data.capitol.texas.gov/dataset/b806b39a-4bab-4103-a66a-9c99bcaba490/resource/c3f03464-e320-4d7f-b528-1883bd82cfb2/download/planc2193_blk.zip\",\n",
    "            \"s\": \"https://data.capitol.texas.gov/dataset/70836384-f10c-423d-a36e-748d7e000872/resource/0af4cde1-f651-4e9f-aded-a47a88cb5548/download/plans2168_blk.zip\",\n",
    "            \"h\": \"https://data.capitol.texas.gov/dataset/71af633c-21bf-42cf-ad48-4fe95593a897/resource/fbba5db7-0b1a-4eee-8d65-375c4d092b62/download/planh2316_blk.zip\",\n",
    "            \"e\": \"https://data.capitol.texas.gov/dataset/ad1ae979-6df9-4322-98cf-6771cc67f02d/resource/714de3c5-753b-4db5-a8b4-b8a768d0cc47/download/plane2106_blk.zip\",\n",
    "        }\n",
    "        L = []\n",
    "        for k, u in U.items():\n",
    "            s = fn.parent / f\"{u.split('/')[-1].split('_')[0].upper()}.csv\"\n",
    "            if not s.exists():\n",
    "                z = self.src / f\"{s.stem}_blk.zip\"\n",
    "                fetch_zip(u, z, fn.parent)\n",
    "            L.append(load(s).set_axis(['geoid20', k+'d'], axis=1).set_index('geoid20'))\n",
    "        return pd.concat(L, axis=1)\n",
    "\n",
    "\n",
    "    def get_vtd_shp(self, fn=None):\n",
    "        s = self.src / f\"VTDs_22G.zip\"\n",
    "        u = f\"https://data.capitol.texas.gov/dataset/4d8298d0-d176-4c19-b174-42837027b73e/resource/037e1de6-a862-49de-ae31-ae609e214972/download/{s.name}\"\n",
    "        fetch(u, s)\n",
    "        return load(s).set_index(['vtdkey','cnty']).rename_axis(['vtd','cnty'])[['geometry']]\n",
    "\n",
    "\n",
    "    def get_blk_shp(self, fn=None):\n",
    "        s = self.src / f\"tl_2020_{self.fips}_tabblock20.zip\"\n",
    "        u = f\"https://www2.census.gov/geo/tiger/TIGER2020/TABBLOCK20/{s.name}\"\n",
    "        fetch(u, s)\n",
    "        return load(s).to_crs(self.vtd_shp.crs).set_index('geoid20')\n",
    "\n",
    "    \n",
    "    def get_blk_vtd(self, fn=None):\n",
    "        V = self.vtd_shp[['geometry']]\n",
    "        B = self.blk_shp[['geometry']]\n",
    "        X = B.sjoin(V, predicate='intersects').set_index(['vtd','cnty'], append=True).join(V, rsuffix='_y')\n",
    "        return X.intersection(X.pop('geometry_y')).area.rename('area').sort_values().reset_index().groupby('geoid20').last()\n",
    "\n",
    "    \n",
    "    # def get_blk_vtd(self, fn=None):\n",
    "    #     B = self.blk_shp[['geometry']].to_crs(self.vtd_shp.crs)\n",
    "    #     V = self.vtd_shp[['geometry']]\n",
    "    #     X = B.sjoin(V, predicate='intersects').set_index(['vtd','cnty'], append=True).join(V, rsuffix='_y')\n",
    "    #     return X.intersection(X.pop('geometry_y')).area.rename('area').sort_values().reset_index().groupby('geoid20').last()\n",
    "\n",
    "    # def get_blk_vtd(self, p=None):\n",
    "    #     B = self.blk_shp['geometry']\n",
    "    #     V = self.vtd_shp['geometry']\n",
    "    #     lst = [B.intersection(v).area.rename('area').reset_index().query('area>0').assign(vtd=k[0], cnty=k[1]) for k, v in V.items()]\n",
    "    #     # lst = [V.intersection(b).area.rename('area').reset_index().query('area>0').assign(geoid20=k) for k, b in B.items()]\n",
    "    #     return pd.concat(lst, ignore_index=True)\n",
    "\n",
    "\n",
    "    def get_blk(self, fn=None):\n",
    "        return (\n",
    "            self.blk_shp[['geometry']]\n",
    "            .join(self.blk_vtd.sort_values('area').groupby('geoid20').last()[['vtd','cnty']])\n",
    "            .join(self.elec.groupby('cnty')['county'].last(), on='cnty')\n",
    "            .join(self.plan)\n",
    "            .assign(leng=lambda x:x.length, area=lambda x:x.area)\n",
    "            .join(self.pl)\n",
    "            .sort_index()\n",
    "        )\n",
    "\n",
    "\n",
    "    def get_blk_simp(self, tol=20, fn=None):\n",
    "        blk = self.blk.copy()\n",
    "        blk.geometry = blk.geometry.buffer(tol).simplify(1).to_crs('EPSG:4326')\n",
    "        return blk\n",
    "\n",
    "\n",
    "    def get_edge(self, df=None, fn=None):\n",
    "        df = self.blk if df is None else df\n",
    "        N = df[['geometry']].rename_axis('idx')\n",
    "        E = (\n",
    "            N.sjoin(N, predicate='intersects')\n",
    "            .set_index('idx_right', append=True)\n",
    "            .query('idx_left < idx_right')\n",
    "            .join(N.rename_axis('idx_right'), rsuffix='_y')\n",
    "        )\n",
    "        return E.intersection(E.pop('geometry_y')).length.rename('leng').rename_axis(['source','destination']).sort_index().reset_index()#.query('leng>0')\n",
    "\n",
    "\n",
    "    def dissolve(self, by):\n",
    "        agg = {k: 'sum' if '_' in k else lambda x: x.mode()[0] for k in self.blk.columns if k != 'geometry'}\n",
    "        return self.blk.dissolve(by, aggfunc=agg).assign(leng=lambda x:x.length, area=lambda x:x.area).sort_index()\n",
    "\n",
    "    def get_cnty(self, fn=None):\n",
    "        return self.dissolve(self.blk['cnty'])\n",
    "\n",
    "    def get_vtd(self, fn=None):\n",
    "        return self.dissolve(self.blk['vtd'])\n",
    "    \n",
    "    def get_cd(self, fn=None):\n",
    "        return self.dissolve(self.blk['cd'])\n",
    "\n",
    "    def get_sd(self, fn=None):\n",
    "        return self.dissolve(self.blk['sd'])\n",
    "\n",
    "    def get_hd(self, fn=None):\n",
    "        return self.dissolve(self.blk['hd'])\n",
    "\n",
    "    def get_ed(self, fn=None):\n",
    "        return self.dissolve(self.blk['ed'])\n",
    "\n",
    "\n",
    "    def get_piece(self, fn=None):\n",
    "        s = fn.parent / f\"tab2010_tab2020_st{self.fips}_{self.st}.txt\"\n",
    "        if not s.exists():\n",
    "            z = self.src / f\"{s.stem.upper()[:-3]}.zip\"\n",
    "            u = f\"https://www2.census.gov/geo/docs/maps-data/data/rel2020/t10t20/{z.name}\"\n",
    "            fetch_zip(u, z, fn.parent)\n",
    "        df = load(s, delimiter='|', usecols=['STATE_2010','STATE_2020','COUNTY_2010','COUNTY_2020','TRACT_2010','TRACT_2020','BLK_2010','BLK_2020','AREALAND_INT']).rename(columns={'arealand_int':'area'})\n",
    "        for x in [10,20]:\n",
    "            dct = {f'state_20{x}':2, f'county_20{x}':3, f'tract_20{x}':6, f'blk_20{x}':4}\n",
    "            for k, v in dct.items():\n",
    "                df[k] = df[k].astype('string').str.rjust(v,'0')\n",
    "            df[f'geoid{x}'] = df[dct.keys()].sum(axis=1)\n",
    "        M = df.groupby(['geoid20','geoid10'])['area'].sum().get_mlt('geoid20')\n",
    "        return M.join(self.blk.drop(columns=['geometry','leng'])).apply_mlt('mlt', self.blk.loc[:,'area':].columns.tolist())\n",
    "\n",
    "\n",
    "    def disagg(self, df, pop, on, div=1):\n",
    "        self.get('piece')\n",
    "        P = self.piece.query(pop+'>0').copy()\n",
    "        P[on] //= div\n",
    "        M = P.groupby(['geoid20',on])[pop].sum().get_mlt(on)\n",
    "        return M.join(df).apply_mlt('mlt').droplevel(on)\n",
    "\n",
    "\n",
    "    def get_vote(self, fn=None):\n",
    "        off = ['president','u.s. sen','governor','lt. governor','land comm','attorney gen','rr comm 1','comptroller']\n",
    "        E = self.elec.query(\"party.isin(['d','r']) & votes>0 & office.isin(@off)\")['votes'].sort_index().unstack('party',0).rename_axis(columns=None)\n",
    "        return self.disagg(E, 'vap_total', 'vtd')\n",
    "\n",
    "\n",
    "    def get_graph(self, fn=None):\n",
    "        self.nodes = cudf.from_pandas(self.blk.drop(columns='geometry'))\n",
    "        self.edges = cudf.from_pandas(self.edge)\n",
    "        self.G = cugr.Graph()\n",
    "        self.G.from_cudf_edgelist(self.edges, edge_attr='leng')\n",
    "\n",
    "        part = self.nodes[self.dst].rename('dst')\n",
    "        pair = self.get_pair(part)[0].to_pandas()\n",
    "        def f(seed):\n",
    "            perm = dict(enumerate(np.random.default_rng(seed).permutation(part.nunique())))\n",
    "            return pair.replace(perm).var(axis=1).sum(), perm\n",
    "        dct = dict(f(seed) for seed in range(100))\n",
    "        self.color = pd.Series(dct[max(dct)]).rename_axis(self.dst).rename('clr').to_frame()\n",
    "\n",
    "\n",
    "    def get_hash(self, part):\n",
    "        Y = sorted(sorted(x) for x in part.to_pandas().groupby('dst').groups.values())\n",
    "        return hashlib.sha256(str(Y).encode()).hexdigest()\n",
    "\n",
    "\n",
    "    def get_pair(self, part, pair=True, pp=True):\n",
    "        E = self.edges.merge(part, left_on='source', right_on='geoid20').merge(part, left_on='destination', right_on='geoid20', suffixes=('','_y'))\n",
    "        mask = E['dst'] == E['dst_y']\n",
    "        L = [None, None]\n",
    "        if pair:\n",
    "            L[0] = E.loc[~mask,['dst','dst_y']].drop_duplicates().sort_index().rename(columns={'dst':'source','dst_y':'destination'})\n",
    "        if pp:\n",
    "            P = self.nodes.join(part).groupby('dst')[['leng','area']].sum()\n",
    "            P -= 2 * E[mask].groupby('dst')[['leng']].sum().assign(area=0)\n",
    "            L[1] = (4*np.pi * P['area'] / P['leng']**2)\n",
    "        return L\n",
    "\n",
    "    \n",
    "    def get_dfct(self, part):\n",
    "        # number of county-district instersections\n",
    "        return self.nodes[['cnty']].join(part).drop_duplicates().groupby('cnty').size()\n",
    "\n",
    "\n",
    "    def get_chain(self, n_step=4, n_root=25, seed=42, fn=None):\n",
    "        fn = self.load_chain(seed)\n",
    "        if isinstance(fn, dict):\n",
    "            return fn  # return results if already available\n",
    "        print(fn)\n",
    "        get_err = lambda v: abs(v-1)\n",
    "        def record(dct):\n",
    "            dct['part'].append(part.squeeze().astype('uint8').sort_index().values_host)\n",
    "            dct['dfct'].append(dfct.sort_index().values_host)\n",
    "            dct['pp'  ].append(pp.sort_index().values_host)\n",
    "            dct['hash'].append(hash)\n",
    "            return dct\n",
    "        part = self.nodes[self.dst].rename('dst').to_frame()\n",
    "        part -= part.min()\n",
    "        dfct = self.get_dfct(part)\n",
    "        pair, pp = self.get_pair(part)\n",
    "        hash = self.get_hash(part)\n",
    "\n",
    "        n_node = self.nodes.shape[0]\n",
    "        n_part = part.nunique().squeeze()\n",
    "        n_cnty = dfct.shape[0]\n",
    "        \n",
    "        pop = self.nodes['all_total']\n",
    "        seat = (pop / pop.sum() * n_part).rename('seat').to_frame()\n",
    "        rec = record({k:[] for k in ['part','dfct','pp','hash']})\n",
    "        fn.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with h5py.File(fn, 'w') as h5f:\n",
    "            h5f.create_dataset('geoid20', data=self.blk.index.values)\n",
    "            h5f.create_dataset('part', shape=[n_step+1, n_node], maxshape=[None, n_node], dtype='uint8')\n",
    "            h5f.create_dataset('dfct', shape=[n_step+1, n_cnty], maxshape=[None, n_cnty], dtype='uint')\n",
    "            h5f.create_dataset('pp'  , shape=[n_step+1, n_part], maxshape=[None, n_part], dtype='float64')\n",
    "            h5f.create_dataset('hash', shape=[n_step+1        ], maxshape=[None        ], dtype='S64')\n",
    "        err_thresh = 0.05\n",
    "        pp_thresh = pp.median()\n",
    "        dfct_thresh = dfct.sum()\n",
    "        head = 0\n",
    "        timer.restart()\n",
    "        while len(rec['hash']) <= n_step:\n",
    "            # dfct_thresh = get_dfct(part)\n",
    "            done = False\n",
    "            for i, pr in pair.sample(frac=1, random_state=seed).T.items():\n",
    "                U = part[part['dst'].isin(pr)]\n",
    "                q = U.join(seat)['seat'].sum()\n",
    "                H = cugr.subgraph(self.G, U.index)\n",
    "                C = cugr.degree_centrality(H)\n",
    "                C = C.sort_values(by=C.columns, ascending=False)\n",
    "                for j in range(n_root):\n",
    "                    u = C.iloc[j,1]\n",
    "                    T = cugr.bfs(H, u)\n",
    "                    T.columns = ['geoid20','dist','ancestor']\n",
    "                    mask = T['dist'] <= 1\n",
    "                    T.loc[mask,'ancestor'] =  T[mask]['geoid20']\n",
    "                    T = T.set_index('geoid20')\n",
    "                    while (T['dist'] > 1).any():\n",
    "                        T = T.merge(T, left_on='ancestor', right_index=True, suffixes=('_x',''))[['dist','ancestor']]\n",
    "                    A = T.join(seat).groupby('ancestor')[['seat']].sum()\n",
    "                    A['other'] = q - A['seat']\n",
    "                    E = get_err(A).max(axis=1)\n",
    "                    for v, err in E[E<err_thresh].to_pandas().items():\n",
    "                        T['new_dst'] = T['ancestor'] == v\n",
    "                        S = T.join(part).groupby(['dst','new_dst']).size().sort_values().reset_index().groupby('dst', sort=False).last()['new_dst']\n",
    "                        S.iloc[0] = ~S.iloc[1]\n",
    "                        P = T.reset_index().merge(S.reset_index()).set_index('geoid20')['dst']\n",
    "                        new_part = part.copy()\n",
    "                        new_part.update(P)\n",
    "                        dfct = self.get_dfct(new_part)\n",
    "                        if dfct.sum() <= dfct_thresh:\n",
    "                            pair, pp = self.get_pair(new_part)\n",
    "                            if pp.median() >= pp_thresh:\n",
    "                                new_hash = self.get_hash(new_part)\n",
    "                                if new_hash != hash:\n",
    "                                    part = new_part\n",
    "                                    hash = new_hash\n",
    "                                    done = True\n",
    "                                    seed += 1\n",
    "                        if done:\n",
    "                            break\n",
    "                    if done:\n",
    "                        break\n",
    "                if done:\n",
    "                    break\n",
    "            if hash in rec['hash']:\n",
    "                print('repeat hash', hash)\n",
    "            else:\n",
    "                rec = record(rec)\n",
    "                tail = len(rec['hash'])\n",
    "                print(\n",
    "                    pattern(tail-1, n_step),\n",
    "                    pattern(j, n_root),\n",
    "                    pattern(err*100, 99.99),\n",
    "                    pattern(dfct.sum(), dfct_thresh),\n",
    "                    pattern(pp.median()*100, 99.99),\n",
    "                    hash,\n",
    "                )\n",
    "                if tail>n_step or tail%500==0:\n",
    "                    # print(head, tail, head-tail)\n",
    "                    with h5py.File(fn, 'a') as h5f:\n",
    "                        for k, v in rec.items():\n",
    "                            h5f[k][head:tail] = v[head-tail:]#.values_host()\n",
    "                            if k != 'hash':\n",
    "                                for x in rec[k]:\n",
    "                                    del x\n",
    "                                rec[k] = []\n",
    "                    head = tail\n",
    "                    t = copy.copy(timer).stop()\n",
    "                    r = t / tail\n",
    "                    s = r*(n_step+1)\n",
    "                    print(f\"{r:.2f} seconds per step = {hf.format_timespan(s)} total with {hf.format_timespan(s-t)} remaining for {n_step} steps\")\n",
    "        print(fn)\n",
    "        return self.load_chain(fn.parts[-2])\n",
    "\n",
    "\n",
    "    def load_chain(self, seed):\n",
    "        fn = self.pq(self.dst, 'chain').parent / f\"{seed}/chain.h5\"\n",
    "        if fn.exists():\n",
    "            dct = {k: pd.DataFrame(v[:].T).squeeze().convert_dtypes() for k, v in load(fn).items()}\n",
    "            dct['geoid20'].rename('geoid20', inplace=True)\n",
    "            dct['part'].set_index(dct['geoid20'], inplace=True)\n",
    "            dct['n_node'] = dct['part'].shape[0]\n",
    "            dct['n_step'] = np.sum(dct['hash'] != '')\n",
    "            dct['seed'] = seed\n",
    "            dct['fn'] = lambda k=None: fn if k is None else fn.with_name(f\"fig{pattern(k, dct['n_step'], 0)}.png\")\n",
    "            return dct\n",
    "        else:\n",
    "            return fn\n",
    "\n",
    "\n",
    "    def plot_map(self, k, save=True, show=True):\n",
    "        self.get('blk_simp')\n",
    "        fn = self.chain['fn'](k)\n",
    "        clr = self.chain['part'][k].replace(self.color.squeeze().to_dict())\n",
    "        s = 16*40/100\n",
    "        self.blk_simp.assign(clr=clr).plot(\n",
    "            figsize=(s,s),\n",
    "            column='clr',\n",
    "            cmap='jet',\n",
    "            linewidth=0,\n",
    "            edgecolor=None,\n",
    "            alpha=1,\n",
    "        )\n",
    "        if save:\n",
    "            plt.savefig(fn)\n",
    "        if show:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "        return fn\n",
    "\n",
    "    def plot_votes(self):\n",
    "        p = self.chain['fn']().parent\n",
    "        print(p)\n",
    "        A = self.chain['part'].assign(**V)\n",
    "        # del self.chain['part']\n",
    "        def f(k):\n",
    "            B = A.groupby(k)[['d','r']].sum().values.T.astype('float')\n",
    "            return B[1] / B.sum(axis=0)\n",
    "        C = np.sort([f(k) for k in range(self.chain['n_step'])], axis=1)\n",
    "        df = pd.DataFrame(C).stack().rename_axis(['step',self.dst]).rename('r').to_frame()\n",
    "        kwargs = {'x':self.dst, 'y':'r', 'orient':'v'}\n",
    "        sns.boxplot(\n",
    "            df.query('step>500'),\n",
    "            fliersize=0,\n",
    "            whis=(5,95),\n",
    "            color='black',\n",
    "            # size=2,\n",
    "            **kwargs,\n",
    "        )    \n",
    "        sns.stripplot(\n",
    "            df.query('step==0'),\n",
    "            size=5,\n",
    "            color='red',\n",
    "            **kwargs,\n",
    "        )\n",
    "        plt.xticks(rotation=-90)\n",
    "        plt.axhline(y=0.5, color='black', linestyle='--')\n",
    "        plt.title('US Congress seed=PLANC2193' if self.dst=='cd' else 'TX Senate seed=PLANS2168' if self.dst=='sd' else 'TX House seed=PLANH2316')\n",
    "        plt.savefig(p / f\"votes_{self.dst}_{seed}.png\")\n",
    "        plt.show()\n",
    "        sns.lineplot(df, x='step', y='r', hue=self.dst, legend=False,\n",
    "                     palette='coolwarm',\n",
    "                    )\n",
    "        plt.savefig(p / f\"converge_{self.dst}_{seed}.png\")\n",
    "        plt.show()\n",
    "\n",
    "# rm(path_out)\n",
    "self = Redistrict(\n",
    "    overwrite = [\n",
    "        # 'src',\n",
    "        # 'pl',\n",
    "        # 'elec',\n",
    "        # 'plan',\n",
    "        # 'vtd_shp',\n",
    "        # 'blk_shp',\n",
    "        # 'blk_vtd',\n",
    "        # 'blk',\n",
    "        # 'cnty',\n",
    "        # 'vtd',\n",
    "        # 'cd',\n",
    "        # 'sd',\n",
    "        # 'hd',\n",
    "        # 'ed',\n",
    "        # 'edge',\n",
    "        # 'piece',\n",
    "        # 'vote',\n",
    "        # 'blk_simp',\n",
    "    ],\n",
    ")\n",
    "# self.get('pl')\n",
    "# self.get('elec')\n",
    "# self.get('plan')\n",
    "# self.get('vtd_shp')\n",
    "# self.get('blk_shp')\n",
    "# self.get('blk_vtd')\n",
    "self.get('blk')\n",
    "# self.get('blk_simp')\n",
    "self.get('edge')\n",
    "# self.get('cnty')\n",
    "# self.get('vtd')\n",
    "# self.get('cd')\n",
    "# self.get('sd')\n",
    "# self.get('hd')\n",
    "# self.get('ed')\n",
    "# self.get('piece')\n",
    "# self.get('vote')\n",
    "# self.get('graph')\n",
    "self.get('vote')\n",
    "\n",
    "for dst in ['cd','hd','sd']:\n",
    "    self = Redistrict(dst=dst)\n",
    "    for seed in [200,300,400]:\n",
    "        print(dst, seed)\n",
    "        if 'chain' in self:\n",
    "            del self.chain\n",
    "        self.get('chain', seed=seed, n_step=1000)\n",
    "        self.plot_votes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeebbda-6f74-491f-b70d-ece30f717fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "for idx in [18]:\n",
    "    if 'chain' in self:\n",
    "        del self.chain\n",
    "    print(idx)\n",
    "    self.get('chain',\n",
    "             n_step=60,\n",
    "             idx = idx,\n",
    "             seed = idx,\n",
    "        )\n",
    "    out = self.chain['fn']().parent / 'animation.mp4'\n",
    "    with imageio.get_writer(out, format='FFMPEG', mode='I', fps=1) as w:\n",
    "        for k in range(self.chain['n_step']):\n",
    "            print(k)\n",
    "            fn = self.plot_(k, show=False)\n",
    "            w.append_data(imageio.v3.imread(fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accd3468-29bd-43da-a686-d1435a601a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio.v3 as iio\n",
    "import imageio\n",
    "out = self.chain['fn']().parent / 'animation.mp4'\n",
    "# frames = np.stack([iio.imread(fn(k)) for k in range(n_steps)], axis=0)\n",
    "frames = [iio.imread(self.chain['fn'](k)) for k in range(n_steps)]\n",
    "# with iio.imopen(out, 'w', plugin='FFMPEG') as writer:\n",
    "#     for img in frames:\n",
    "#     # for img_file in image_files:\n",
    "#     #     img = iio.imread(img_file)  # Read each image\n",
    "#         writer.write(img)  # Write each image to the video\n",
    "\n",
    "w = imageio.get_writer(out, format='FFMPEG', mode='I', fps=1,\n",
    "                       # codec='h264_vaapi',\n",
    "                       # output_params=['-vaapi_device',\n",
    "                       #                '/dev/dri/renderD128',\n",
    "                       #                '-vf',\n",
    "                       #                'format=gray|nv12,hwupload'],\n",
    "                       # pixelformat='vaapi_vld',\n",
    "                      )\n",
    "for fr in frames:\n",
    "    w.append_data(fr)\n",
    "# w.append_data(image2)\n",
    "# w.append_data(image3)\n",
    "w.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f8bd6b-b336-4be8-be87-794dc7e67c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "part = self.nodes[self.dst].rename('dst')\n",
    "pairs, pp = self.get_pairs(part)\n",
    "H = nx.from_pandas_edgelist(pairs.to_pandas(), target='destination')\n",
    "clr = nx.greedy_color(H)\n",
    "n_colors = 2*len(set(clr.values()))\n",
    "clr = nx.equitable_color(H, n_colors)\n",
    "B = self.blk_simp.assign(clr=C[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e19acd8-82af-4f83-8c68-16e5a91cf3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import imageio\n",
    "\n",
    "out = fn(0).parent / 'animation.gif'\n",
    "frames = [imageio.imread(fn(k)) for k in range(n_steps)]\n",
    "imageio.mimsave(out, frames, duration=0.5)\n",
    "\n",
    "# images = [fn(k) for k in range(n_steps)]\n",
    "# frames = []\n",
    "# for image in images:\n",
    "#     # img_path = os.path.join(image_folder, image)\n",
    "#     frames.append(imageio.imread(img_path))\n",
    "\n",
    "# def create_animation(image_folder, output_file, duration=0.5):\n",
    "#     # Get list of image files in the folder\n",
    "    \n",
    "#         # img for img in os.listdir(image_folder) if img.endswith(\".png\") or img.endswith(\".jpg\")]\n",
    "#     # images.sort()  # Sort images by name\n",
    "\n",
    "#     # Read images and store them in a list\n",
    "#     frames = []\n",
    "#     for image in images:\n",
    "#         # img_path = os.path.join(image_folder, image)\n",
    "#         frames.append(imageio.imread(img_path))\n",
    "\n",
    "#     # Create and save the animation\n",
    "#     imageio.mimsave(output_file, frames, duration=duration)\n",
    "\n",
    "#     print(f\"Animation saved as {output_file}.\")\n",
    "\n",
    "# # Example usage\n",
    "# image_folder = 'path_to_image_folder'  # Replace with the path to your folder containing images\n",
    "# output_file = 'animation.gif'  # Output file name\n",
    "# create_animation(image_folder, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a32b0f-8e01-457c-bd99-ec98e6efd8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "from PIL import Image\n",
    "fig, ax = plt.subplots()\n",
    "img = Image.open(fn(0))\n",
    "im = ax.imshow(img)\n",
    "\n",
    "def update(k):\n",
    "    ax.clear()\n",
    "    img = Image.open(fn(k))\n",
    "    ax.imshow(img)\n",
    "    # im.set_array(img)\n",
    "    # return [im]\n",
    "ani = FuncAnimation(fig, update, frames=n_steps)#, interval=200)  # interval in milliseconds\n",
    "# ani.save('animation.mp4', writer='ffmpeg')\n",
    "ani.save(fn(0).parent / 'animation.gif', writer='pillow', fps=5)  # Adjust fps as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d6bf9e-9070-497d-9042-282bde7a88a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df = px.data.election()\n",
    "# geo_df = gpd.GeoDataFrame.from_features(\n",
    "#     px.data.election_geojson()[\"features\"]\n",
    "# ).merge(df, on=\"district\").set_index(\"district\")\n",
    "fig = px.choropleth(geo_df,\n",
    "                   geojson=geo_df.geometry,\n",
    "                   locations=geo_df.index,\n",
    "                   # color=\"Joly\",\n",
    "                   projection=\"mercator\")\n",
    "# fig.update_geos(fitbounds=\"locations\", visible=False)\n",
    "# fig.show()\n",
    "# fig = px.choropleth_map(geo_df,\n",
    "#                            geojson=geo_df.geometry,\n",
    "#                            locations=geo_df.index,\n",
    "#                            # color=\"Joly\",\n",
    "#                            # center={\"lat\": 45.5517, \"lon\": -73.7073},\n",
    "#                            # map_style=\"open-street-map\",\n",
    "#                            # title='World Population Estimates',\n",
    "#                            # zoom=8.5\n",
    "#                    )\n",
    "# fig.update_geos(fitbounds=\"locations\")#, visible=False)\n",
    "fig.write_html('/home/pythonserver/gerrymandering/test.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fd4588-4acf-42c5-8782-63cadc644c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import plotly.express as px\n",
    "\n",
    "# Load a GeoDataFrame (example: world countries)\n",
    "gdf = gpd.read_file(path_root / 'ne_110m_admin_1_states_provinces.zip')\n",
    "    # gpd.datasets.get_path('naturalearth_lowres'))\n",
    "\n",
    "# Add a column with random data for demonstration\n",
    "gdf['value'] = 5#gdf['pop_est']  # Example: using population estimates\n",
    "\n",
    "# Create a choropleth map\n",
    "fig = px.choropleth(\n",
    "    gdf,\n",
    "    geojson=gdf.geometry.__geo_interface__,\n",
    "    locations=gdf.index,\n",
    "    color='value',\n",
    "    hover_name='name',  # Country names for hover\n",
    "    color_continuous_scale=px.colors.sequential.Plasma,\n",
    "    title='World Population Estimates',\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_geos(fitbounds=\"locations\", visible=False)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc04a4b0-c84c-4da5-8e8e-b70c3b529ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "cm = 'twilight'\n",
    "fig, ax = plt.subplots()\n",
    "# im = X[0].plot('clr', cmap=cm)\n",
    "def generate_image(k):\n",
    "    ax.clear()\n",
    "    Y[k].plot('clr', cmap=cm, ax=ax)\n",
    "    return ax\n",
    "ani = animation.FuncAnimation(fig, generate_image, frames=3)#, interval=100, blit=True)\n",
    "# ani.save(path_root / 'test.gif', writer='pillow', fps=3)\n",
    "ani.save(path_root / 'test.mp4', writer='ffmpeg', fps=10)\n",
    "\n",
    "# anim = animation.FuncAnimation(\n",
    "#     fig,\n",
    "#     generate_image,\n",
    "#     frames=2,\n",
    "#     # interval=100,\n",
    "#     # blit=True,\n",
    "# )\n",
    "# anim.save(fn.parent / 'animation.gif', writer='pillow')\n",
    "# # plt.show()\n",
    "# # for k, p in P.items():\n",
    "# #     fig_fn = fn.parent / pattern(k, max(P.columns), 0)\n",
    "# #     print(fig_fn)\n",
    "# #     X = self.dissolve(p).assign(clr=clr)\n",
    "# #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09386d8f-6148-418e-b8fc-cb47f070af20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Set up the grid for the plot (meshgrid)\n",
    "x = np.linspace(-2, 2, 200)\n",
    "y = np.linspace(-2, 2, 200)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# Initialize the image to be displayed\n",
    "im = ax.imshow(np.zeros((200, 200)), cmap='inferno', vmin=0, vmax=1)\n",
    "\n",
    "# Function to generate a dynamic expanding circle image\n",
    "def generate_image(frame):\n",
    "    radius = 0.1 + frame * 0.05  # The radius increases over time\n",
    "    distance_from_center = np.sqrt(X**2 + Y**2)  # Distance of each point from the center\n",
    "    image_data = np.where(distance_from_center < radius, 1, 0)  # Set points inside the circle to 1\n",
    "    im.set_array(image_data)  # Update the image data in the plot\n",
    "    return [im]\n",
    "\n",
    "# Create the animation\n",
    "ani = animation.FuncAnimation(fig, generate_image, frames=50, interval=100, blit=True)\n",
    "\n",
    "# Save the animation as a GIF\n",
    "\n",
    "ani.save(path_root / 'expanding_circle.gif', writer='pillow', fps=10)\n",
    "\n",
    "print(\"Animation saved as expanding_circle.gif\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4d8024-56c8-4195-9662-5d622e68b02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "# Load your GeoDataFrame (for example, a shapefile)\n",
    "# Replace this with your actual GeoDataFrame\n",
    "# gdf= gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "gdf = self.vtd_shp\n",
    "\n",
    "# Example dynamic data: Simulate some changing data (e.g., population growth)\n",
    "time_steps = 10\n",
    "gdf['initial_data'] = gdf['pop_est']  # Use population estimate as the initial data\n",
    "gdf_list = [gdf.copy() for _ in range(time_steps)]  # Copy the GeoDataFrame for each time step\n",
    "\n",
    "# Simulate data changes over time (for demonstration)\n",
    "for i, gdf in enumerate(gdf_list):\n",
    "    gdf['data'] = gdf['initial_data'] * (1 + 0.1 * i)  # Increase data by 10% each step\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Function to update the choropleth plot for each frame\n",
    "def update(frame):\n",
    "    ax.clear()  # Clear the previous frame's plot\n",
    "    gdf_list[frame].plot(column='data', ax=ax, legend=True, cmap='viridis', \n",
    "                         legend_kwds={'label': \"Population\", 'orientation': \"horizontal\"})\n",
    "    ax.set_title(f\"Time Step {frame + 1}\")  # Update the title with the current time step\n",
    "    return ax\n",
    "\n",
    "# Create the animation\n",
    "ani = animation.FuncAnimation(fig, update, frames=time_steps, interval=500, repeat=True)\n",
    "\n",
    "# Save the animation as a GIF\n",
    "ani.save(path_root / 'choropleth_animation.gif', writer='pillow', fps=2)\n",
    "\n",
    "print(\"Choropleth animation saved as 'choropleth_animation.gif'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fde0041-01da-4235-9b36-c1600160b87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = path_out / \"tx/src/VTDs_22G.zip\"\n",
    "w.is_file()\n",
    "gdf = load(w)\n",
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d5de55-a690-4f3a-9270-be0b065be2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "# Load the GeoDataFrame (from a local shapefile)\n",
    "# Replace this with your actual shapefile path\n",
    "gdf = gpd.read_file(path_out / \"tx/src/VTDs_22G.zip\")\n",
    "\n",
    "# Example dynamic data: Simulate some changing data (e.g., population growth)\n",
    "time_steps = 10\n",
    "gdf['initial_data'] = gdf['pop_est']  # Use population estimate or another column as the initial data\n",
    "gdf_list = [gdf.copy() for _ in range(time_steps)]  # Copy the GeoDataFrame for each time step\n",
    "\n",
    "# Simulate data changes over time (for demonstration)\n",
    "for i, gdf in enumerate(gdf_list):\n",
    "    gdf['data'] = gdf['initial_data'] * (1 + 0.1 * i)  # Increase data by 10% each step\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Function to update the choropleth plot for each frame\n",
    "def update(frame):\n",
    "    ax.clear()  # Clear the previous frame's plot\n",
    "    gdf_list[frame].plot(column='data', ax=ax, legend=True, cmap='viridis', \n",
    "                         legend_kwds={'label': \"Population\", 'orientation': \"horizontal\"})\n",
    "    ax.set_title(f\"Time Step {frame + 1}\")  # Update the title with the current time step\n",
    "    return ax\n",
    "\n",
    "# Create the animation\n",
    "ani = animation.FuncAnimation(fig, update, frames=time_steps, interval=500, repeat=True)\n",
    "\n",
    "# Save the animation as a GIF\n",
    "ani.save('choropleth_animation.gif', writer='pillow', fps=2)\n",
    "\n",
    "print(\"Choropleth animation saved as 'choropleth_animation.gif'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6697c1b3-d70a-453d-b495-77f5a9081450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Initialize blank image\n",
    "x = np.linspace(-2, 2, 100)\n",
    "y = np.linspace(-2, 2, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "im = ax.imshow(np.zeros((100, 100)), cmap='inferno')\n",
    "\n",
    "# Function to generate a dynamic expanding circle\n",
    "def generate_image(frame):\n",
    "    radius = 0.1 + frame * 0.05  # Increase radius over time\n",
    "    circle = np.sqrt(X**2 + Y**2)  # Distance from the center\n",
    "    image_data = np.where(circle < radius, 1, 0)  # Set pixels inside the circle to 1\n",
    "    im.set_array(image_data)\n",
    "    return [im]\n",
    "\n",
    "# Create the animation\n",
    "ani = animation.FuncAnimation(fig, generate_image, frames=100, interval=100, blit=True)\n",
    "\n",
    "# Show the animation\n",
    "# plt.show()\n",
    "ani.save(path_root / 'expanding_circle.mp4', writer='ffmpeg', fps=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35bb36b-2ed0-4a95-8ad2-59893612af80",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn.parent / f\"{pattern(k, max(P.columns), 0)}.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085beda3-f51e-4594-acc9-b6eb880b1888",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmaps = [\n",
    "    'twilight',\n",
    "    # 'spectral',\n",
    "    'brg',\n",
    "    'coolwarm',\n",
    "    'flag',\n",
    "    'gist_ncar',\n",
    "    'gist_rainbow',\n",
    "    'gnuplot',\n",
    "    'hsv',\n",
    "    'jet',\n",
    "    'nipy_spectral',\n",
    "    'rainbow',\n",
    "    'tab20b',\n",
    "    'turbo',\n",
    "]\n",
    "# for cm in cmaps:\n",
    "# # for cmap in plt.colormaps():\n",
    "#     print(cm)\n",
    "#     R.plot('clr', cmap=cm)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc03950a-713a-459e-a888-998fe30fbf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "R = Q[['geometry']]\n",
    "\n",
    "\n",
    "E = R.sjoin(R, predicate='intersects').reset_index().set_axis(['source','geo','target'],axis=1)\n",
    "G = nx.from_pandas_edgelist(E)\n",
    "color = nx.equitable_color(G,20)\n",
    "color = pd.Series(color).rename('color')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7395ab-c6e2-43b0-bf25-33ca6cb80553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cmap in plt.colormaps():\n",
    "#     print(cmap)\n",
    "#     R.plot(cmap=cmap)\n",
    "#     plt.show()\n",
    "    # break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids",
   "language": "python",
   "name": "rapids"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
